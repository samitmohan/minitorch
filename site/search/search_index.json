{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":"<p>A minimalist PyTorch\u2013style deep learning engine built from scratch</p> <p></p>"},{"location":"#overview","title":"Overview","text":"<p>MiniTorch is a small-footprint deep\u2010learning framework implemented entirely in Python. It provides:</p> <ul> <li><code>Tensor</code>: a NumPy-backed tensor with full broadcasting and automatic gradient tracking.  </li> <li><code>Linear</code>, <code>ReLU</code>, <code>BatchNorm1d</code>: fundamental layer types.  </li> <li>Loss functions: Mean Squared Error (MSE) and Cross-Entropy.  </li> <li>Optimizers: SGD and Adam.  </li> <li>Demos: a toy regression (learn y = 2x + 1), a small MNIST example, and benchmarking scripts.</li> </ul>"},{"location":"#quickstart","title":"Quickstart","text":"<pre><code>from minitorch import Tensor, Linear, SGD, mse_loss\n\n# Sample data\nx = Tensor([[1.0], [2.0], [3.0]], requires_grad=False)\ny = Tensor([[3.0], [5.0], [7.0]], requires_grad=False)\n\n# Build model\nmodel = [Linear(1, 1)]\nparams = []\nfor layer in model:\n    params += layer.parameters()\n\noptimizer = SGD(params, lr=0.1)\n\n# Training loop\nfor epoch in range(5):\n    pred = model[0](x)\n    loss = mse_loss(pred, y)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    print(f\"Epoch {epoch}, Loss {loss.data:.4f}\")\n</code></pre>"},{"location":"tutorial/","title":"Tutorial &amp; Use Cases","text":""},{"location":"tutorial/#regression-demo","title":"Regression Demo","text":"<p>Learn a simple linear relationship y = 2x + 1:</p> <pre><code>python train.py\n</code></pre> <p>This runs 100 epochs on synthetic data and plots training loss.</p>"},{"location":"tutorial/#mnist-demo","title":"MNIST Demo","text":"<p>Train a small 2-Layer neural network on MNIST: <pre><code>python mnist_example.py\n</code></pre></p>"},{"location":"tutorial/#benchmarking","title":"Benchmarking","text":"<p>Compare MiniTorch performance to NumPy and PyTorch:</p> <pre><code>python benchmark.py\n</code></pre> <p>See timing output in the console.</p>"},{"location":"api/layers/","title":"Layers","text":""},{"location":"api/layers/#linearin_features-out_features-biastrue","title":"<code>Linear(in_features, out_features, bias=True)</code>","text":"<p>Fully-connected layer:</p> <pre><code>from minitorch.layers import Linear\nfrom minitorch import Tensor\n\nlayer = Linear(5, 2)\nx = Tensor.zeros((3,5), requires_grad=True)\ny = layer(x)\n</code></pre>"},{"location":"api/layers/#relu","title":"<code>ReLU()</code>","text":"<p>Activation layer:</p> <pre><code>from minitorch.layers import ReLU\nact = ReLU()\ny = act(x)\n</code></pre>"},{"location":"api/layers/#batchnorm1dnum_features","title":"<code>BatchNorm1d(num_features)</code>","text":"<p>Batch normalization for 1D features. TODO: _backward function</p>"},{"location":"api/loss/","title":"Loss Functions","text":""},{"location":"api/loss/#mse_lossinput-target","title":"<code>mse_loss(input, target)</code>","text":"<p>Mean Squared Error:</p> <pre><code>from minitorch.loss import mse_loss\nloss = mse_loss(predictions, targets)\n</code></pre>"},{"location":"api/loss/#cross_entropy_lossinput-target","title":"<code>cross_entropy_loss(input, target)</code>","text":"<p>Cross-entropy for classification.</p> <pre><code>from minitorch.loss import cross_entropy_loss\nloss = cross_entropy_loss(predictions, targets)\n</code></pre>"},{"location":"api/optim/","title":"Optimizers","text":""},{"location":"api/optim/#sgdparams-lr-momentum00","title":"<code>SGD(params, lr, momentum=0.0)</code>","text":"<p>Stochastic Gradient Descent with momentum:</p> <pre><code>from minitorch.optim import SGD\nopt = SGD(model.parameters(), lr=0.01, momentum=0.9)\nopt.zero_grad()\nloss.backward()\nopt.step()\n</code></pre>"},{"location":"api/optim/#adamparams-lr-betas090999-eps1e-8","title":"<code>Adam(params, lr, betas=(0.9,0.999), eps=1e-8)</code>","text":"<p>Adam optimizer. <pre><code>from minitorch.optim import Adam\nopt = Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-8)\nopt.zero_grad()\nloss.backward()\nopt.step()\n</code></pre></p>"},{"location":"api/tensor/","title":"Tensor","text":"<p>The core <code>Tensor</code> class wraps a NumPy array and tracks gradients for backpropagation.</p>"},{"location":"api/tensor/#key-methods","title":"Key Methods","text":"<ul> <li><code>Tensor(data, requires_grad=False)</code>: create a tensor.</li> <li><code>tensor.sum(axis=None, keepdims=False)</code>: sum elements.</li> <li><code>tensor.reshape(*shape)</code>: reshape tensor.</li> <li><code>tensor.backward()</code>: compute gradients.</li> </ul>"},{"location":"api/tensor/#example","title":"Example","text":"<pre><code>from minitorch import Tensor\n\nx = Tensor([[2.0, 0.0, -2.0]], requires_grad=True)\nI = Tensor.eye(3, requires_grad=True)\nz = x.matmul(I).sum()\nz.backward()\n\nprint(I.grad)\nprint(x.grad)\n</code></pre>"}]}